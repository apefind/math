\newpage
\section{Neural Networks}

\subsection{The Perceptron}

\begin{definition}[Activation Functions]\hfill
    \begin{enumerate}
		\item The \emph{heaviside} function \( H: \R \to \{ 0, 1 \} \) is defined as
			\[
				H(x) = \left\{
					\begin{array}{ll}
						1 & x > 0 \\
						0 & x \le 0 \\
					\end{array} 
				\right.
			\]
		\item The \emph{sigmoid} function \( \sigmoid \in C^\infty(\R) \) is defined as 
			\[
				\sigmoid(x) = \frac{1}{1 + e^{-x}}
			\]
    \end{enumerate}
\end{definition}
\bigskip


\begin{figure}[H]
	\centering
	\plotsigmoid
	\caption{The sigmoid function $ \sigmoid(x) = \frac{1}{1 + e^{-x}} $}\label{fig:sigmoid}
\end{figure}
\bigskip


\begin{remarks}\hfill
    \begin{enumerate}
		\item Since the heaviside function is not continous and therefore not differentiable at \( 0 \)
			the sigmoid function is often considered its smooth counterpart
		\item The definition of the sigmoid function yields \( 0 < \sigmoid(x) <  1 \) as well as 
			\( \sigmoid(x) \to 0 \) for \( x \to -\infty \) and \( \sigmoid(x) \to 1 \) for \( x \to \infty \)
		\item The quotient rule yields
			\[
				\sigmoid'(x) 
					= -\frac{-e^{-x}}{(1 + e^{-x})^2}
					= \sigmoid(x) \frac{1 + e^{-x} - 1}{1 + e^{-x}}
					= \sigmoid(x)(1 - \sigmoid(x))
			\]
			and \( \sigmoid \) is monotonically increasing over its domain
    \end{enumerate}
\end{remarks}
\bigskip


\begin{definition}[Binary Classifiers]\hfill
    \begin{enumerate}
		\item Let \( M \subset \R^n \). A mapping \( f: M \to \{ 0, 1 \} \) is called a \emph{binary classifier} 
		\item A \emph{weight} \( w \in \R^n \) and a \emph{bias} \( b \in \R \) define \emph{binary classifier} via
			\[
				f(x) = H(wx + b) = H(\sum_{i=1}^n x_i w_i + b)
			\]
		\item Let \( f \) be a binary classifier. A weight \( w \in \R^n \) and a bias \( b \) are called a
			\emph{solution for the classification problem} if 
				\[
					H(wx + b) = f(x)
				\]
			for \( x \in M \)
    \end{enumerate}
\end{definition}
\bigskip


\begin{examples}\hfill
    \begin{enumerate}
        \item Let \( M = \{ 0, 1 \} \times \{ 0, 1 \} \) and consider the \emph{and} operator 
		    \( f(1, 1) = 1 \) and \( f(x, y) = 0 \) elsewhere. Then \( w = (3, 3) \) and \( b = -5 \) 
			yields a solution
        \item Again let \( M = \{ 0, 1 \} \times \{ 0, 1 \} \) and \( f(1, 0) = f(0, 1) = 1 \) and 
			\( f(0, 0) = f(1, 1) = 0 \), the \emph{xor} operator. Thus
				\[
					\begin{split}
						w_1 + b & > 0 \\
						w_2 + b & > 0 \\
						w_1 + w_2 + b & \le 0 \\
						b & \le 0 \\
					\end{split}
				\]
			Adding two equations respectively shows that there cannot be a solution
	\end{enumerate}
\end{examples}
\bigskip


\begin{algorithm}[Perceptron]\label{algo:perceptron}
\end{algorithm}
% \inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{perceptron.py}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/perceptron.py}
\bigskip


\subsection{The Backtracking Algorithm}


