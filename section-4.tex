\newpage
\section{Neural Networks}

\subsection{The Perceptron}

\begin{definition}[Activation Functions]\hfill
    \begin{enumerate}
		\item The \emph{heaviside} function \( H: \R \to \{ 0, 1 \} \) is defined as
			\[
				H(x) = \left\{
					\begin{array}{ll}
						1 & x > 0 \\
						0 & x \le 0 \\
					\end{array} 
				\right.
			\]
		\item The \emph{sigmoid} function \( \sigmoid \in C^\infty(\R) \) is defined as 
			\[
				\sigmoid(x) = \frac{1}{1 + e^{-x}}
			\]
    \end{enumerate}
\end{definition}
\bigskip


\begin{figure}[H]
	\centering
	\plotsigmoid
	\caption{The sigmoid function $ \sigmoid(x) = \frac{1}{1 + e^{-x}} $}\label{fig:sigmoid}
\end{figure}
\bigskip


\begin{remarks}\hfill
    \begin{enumerate}
		\item Since the heaviside function is not continous and therefore not differentiable at \( 0 \), 
			so the sigmoid function is often considered as its smooth counterpart
		\item The definition of the sigmoid function yields \( 0 < \sigmoid(x) <  1 \) as well as 
			\( \sigmoid(x) \to 0 \) for \( x \to -\infty \) and \( \sigmoid(x) \to 1 \) for \( x \to \infty \)
		\item The quotient rule yields
			\[
				\sigmoid'(x) 
					= -\frac{-e^{-x}}{(1 + e^{-x})^2}
					= \sigmoid(x) \frac{1 + e^{-x} - 1}{1 + e^{-x}}
					= \sigmoid(x)(1 - \sigmoid(x))
			\]
			and \( \sigmoid \) is monotonically increasing over its domain
    \end{enumerate}
\end{remarks}
\bigskip


\begin{definition}[Binary Classification]
Let \( M = \{ x_0, x_1, \dots x_m \} \subset \R^n \) and \( f: M \to \{ 0, 1 \} \).
A vector \( w = (w_1, w_2, \dots, w_n) \in \R^n \) and \( b \in \R \) is called a \emph{binary classifier} 
	\[
		f(x) = H(wx + b) = H(\sum_{i=1}^n x_i w_i + b)
	\]
for all \( x \in M \).
\end{definition}
\bigskip


\begin{examples}\hfill
    \begin{enumerate}
        \item Let \( M = \{ 0, 1 \} \times \{ 0, 1 \} \) and consider the \emph{and} operator 
		    \( f(1, 1) = 1 \) and \( f(x, y) = 0 \) elsewhere. Then \( w = (3, 3) \) and \( b = -5 \) 
			yields a solution
        \item Again let \( M = \{ 0, 1 \} \times \{ 0, 1 \} \) and \( f(1, 0) = f(0, 1) = 1 \) and 
			\( f(0, 0) = f(1, 1) = 0 \), the \emph{xor} operator. Thus
				\[
					\begin{split}
						w_1 + b & > 0 \\
						w_2 + b & > 0 \\
						w_1 + w_2 + b & \le 0 \\
						b & \le 0 \\
					\end{split}
				\]
			Adding two equations respectively shows that there cannot be a solution
	\end{enumerate}
\end{examples}
\bigskip


\begin{algorithm}[Perceptron]\label{algo:perceptron}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{perceptron.py}
\bigskip


\subsection{The Backtracking Algorithm}


