\newpage
\section{Neural Networks}

\subsection{The Perceptron}

\begin{definition}[Binary Classifiers]
    Let \( X \subset \R^n \) be the union of two finite disjoint sets \( X = M \cup N \).
    \begin{enumerate}
        \item A \emph{binary classification problem} is the task to find a mapping \( f: X \to \{ 0, 1 \} \) with
              \[
                  f(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ for } x \in M \\
                      0 & \text{ for } x \in N
                  \end{array}
                  \right.
              \]
              \( f \) then is called a \emph{binary classifier} for \( X \)
        \item \( X \) is called \emph{separable} if there exists a \emph{weight vector} \( w \in \R^n \)
              and a \emph{bias} \( b \in \R \) so that
              \[
                  \begin{split}
                      wx + b & > 0 \hspace{1em}\text{ for } x \in M \\
                      wx + b & < 0 \hspace{1em}\text{ for } x \in N
                  \end{split}
              \]
        \item The weight \( w \) and the bias \( b \) are called \emph{solution to the classification problem}.
              They implicitly define a binary classifier via
              \[
                  f(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ if } wx + b > 0 \\
                      0 & \text{ if } wx + b < 0
                  \end{array}
                  \right.
              \]
    \end{enumerate}
\end{definition}
\bigskip


\begin{examples}
    \hfill
    \begin{enumerate}
        \item Let \( X = \{ 0, 1 \} \times \{ 0, 1 \} \) and consider the \emph{and} operator
              \( f(1, 1) = 1 \) and \( f(x, y) = 0 \) elsewhere. Then \( w = (3, 3) \) and \( b = -5 \)
              yield a solution to the classification problem \( M = f^{-1}(1) \) and \( N = f^{-1}(0) \)
        \item Again let \( X = \{ 0, 1 \} \times \{ 0, 1 \} \) and \( f(1, 0) = f(0, 1) = 1 \) and
              \( f(0, 0) = f(1, 1) = 0 \), the \emph{xor} operator. Thus for any weight \( (w_1, w_2) \) and
              any bias \( b \)
              \[
                  \begin{split}
                      w_1 + b & > 0 \\
                      w_2 + b & > 0 \\
                      \\
                      w_1 + w_2 + b & \le 0 \\
                      b & \le 0
                  \end{split}
              \]
              Adding two equations respectively shows that there is no solution
        \item The bias can be integrated into the weight vector via \( w' = (w, b) \in \R^{n + 1} \) and
              \( x' = (x, 1) \in \R^{n + 1} \). Separability then reduces to
              \[
                  w'x' > 0
              \]
    \end{enumerate}
\end{examples}
\bigskip


\subsubsection*{Geometrical Interpretation}
The idea for the perceptron most likely has its origin in a simple geometrical observation.
Recall that for \( x, y \in \R^n \) the dot product can be expressed as
\[
    xy = \|x\| \|y\| \cos\alpha
\]
where \( \alpha \) is the angle between the two vectors. Hence the product is positive
if the angle is less than \( \ang{90} \) degrees and negative if the angle is between \( \ang{90} \)
and \( \ang{180} \) degrees
\[
    \begin{split}
        xy & > 0 \hspace{1em}\text{ for } 0 \le \alpha < \pi / 2 \\
        xy & < 0 \hspace{1em}\text{ for } \pi / 2 < \alpha \le \pi
    \end{split}
\]
Note, that the sign does not depend on the vector lengths, but solely on the angle.

For any two vectors it is easy enough to find a weight that satisfies \( wx > 0 \) and \( wy > 0 \).
Generally \( w = x + y \) is a good guess, but not always correct as shown below in
\hyperref[fig:vectorangle]{Figure~\ref*{fig:vectorangle}}.

\bigskip
\begin{figure}[H]
    \centering
    \plotvectorangle{}
    \caption{Dot Product and Angle}\label{fig:vectorangle}
\end{figure}
\bigskip

But, the more similar the lengths of the two vectors are the more likely \( x + y \) works.
An iterative approach is to increase \( w = x + y \) in the direction of vector with the angle greater
than \( \ang{90} \) degrees.
\[
    w_{n + 1} = \left \{
    \begin{array}{ll}
        w_n + x & \text{ if } w_n x < 0 \\
        w_n + y & \text{ if } w_n y < 0 \\
    \end{array}
    \right.
\]
\bigskip

\begin{examples}
    \hfill
    \begin{enumerate}
        \item Let \( x = (-1, 1) \) and \( y = (6, 1) \). Then
              \[
                  \begin{aligned}
                      w_0 & = (5, 2) & w_0x & = -3 & w_0y & = 32 \\
                      w_1 & = (4, 3) & w_1x & = -1 & w_1y & = 27 \\
                      w_2 & = (3, 4) & w_2x & = 1  & w_2y & = 22 \\
                  \end{aligned}
              \]
        \item Let \( x = (4, -6) \) and \( y = (-10, 5) \). Then
              \[
                  \begin{aligned}
                      w_0 & = (-6, -1)  & w_0x & = -18 & w_0y & = 55  \\
                      w_1 & = (-2, -7)  & w_1x & = 34  & w_1y & = -15 \\
                      w_2 & = (-12, -2) & w_2x & = -36 & w_2y & = 110 \\
                      w_3 & = (-8, -8)  & w_3x & = 16  & w_3y & = 40  \\
                  \end{aligned}
              \]
        \item Let \( \|x\| < \|y\| \) and \( \cos\alpha < 0 \). Then
              \[
                  \begin{split}
                      x(x + y) & = xx + xy = {\|x\|}^2 + \|x\| \|y\| \cos\alpha \\
                      x(x + y) & = \|x\| \|x + y\| \cos\beta
                  \end{split}
              \]
              and
              \[
                  {\|x + y\|}\cos\beta = \|x\| + \|y\|\cos\alpha > 0
              \]
              Hence \( \cos\beta > 0 \) if
              \[
                  \|x\| > -\|y\|\cos\alpha
              \] 
    \end{enumerate}
\end{examples}
\bigskip

\begin{algorithm}[Weight]\label{algo:weight}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/weight.py}
\bigskip



\begin{algorithm}[Perceptron]\label{algo:perceptron}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/perceptron.py}
\bigskip


\subsection{The Backtracking Algorithm}

\begin{definition}[Activation Functions]
    \hfill
    \begin{enumerate}
        \item The \emph{Heaviside} function \( H: \R \to \{ 0, 1 \} \) is defined as
              \[
                  H(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ for } x > 0  \\
                      0 & \text{ for }x \le 0
                  \end{array}
                  \right.
              \]
        \item The \emph{sigmoid} function \( \sigmoid \in C^\infty(\R) \) is defined as
              \[
                  \sigmoid(x) = \frac{1}{1 + e^{-x}}
              \]
    \end{enumerate}
\end{definition}
\bigskip


\begin{figure}[H]
    \centering
    \plotsigmoid{}
    \caption{The sigmoid function \( \sigmoid(x) = \frac{1}{1 + e^{-x}} \)}\label{fig:sigmoid}
\end{figure}
\bigskip


\begin{remarks}
    \hfill
    \begin{enumerate}
        \item Since the heaviside function is not continuous and therefore not differentiable at \( 0 \)
              the sigmoid function is often considered its smooth counterpart
        \item The definition of the sigmoid function yields \( 0 < \sigmoid(x) < 1 \) as well as
              \( \sigmoid(x) \to 0 \) for \( x \to -\infty \) and \( \sigmoid(x) \to 1 \) for \( x \to \infty \)
        \item The quotient rule yields
              \[
                  \sigmoid'(x)
                  = -\frac{-e^{-x}}{{(1 + e^{-x})}^2}
                  = \sigmoid(x) \frac{1 + e^{-x} - 1}{1 + e^{-x}}
                  = \sigmoid(x)(1 - \sigmoid(x))
              \]
              and \( \sigmoid \) is monotonically increasing over its domain
    \end{enumerate}
\end{remarks}
\bigskip
