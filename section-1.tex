
\newpage
\section{Minimization without Constraints}

Alternative proof of Lemma 2.10.

\lemma{}\label{lemma_gradient_inequality}
Let \(M \subseteq \mathbb{R}^n\) be a convex set and \(f \in C^1(M)\). Then \(f\) is convex if and only if
\[
    f(x) \ge f(y) + \gradient {f(y)}^T (x - y)
\]
for all \(x, y \in M \). 

\proof{}
Let f be convex and \(x, y \in M\). For \( 0 \le \lambda \le 1 \) we have
\[ 
    f(\lambda x + (1 - \lambda) y) \le \lambda f(x) + (1 - \lambda)f(y) =  \lambda f(x) - \lambda f(y) + f(y) 
\] 
and 
\[ 
    f(x) - f(y) \ge \frac{f(\lambda x + (1 - \lambda) y) - f(y)}{\lambda}
        = \frac{f(y + \lambda (x - y)) - f(y)}{\lambda}
\]
For \( d = x - y \)\ and \( \lambda \to 0 \) the term on the right converges to the direction derivative of \( f \)
in \( d \)
\[
    \frac{\partial f}{\partial d}(y) = \gradient{f(y)}^T d = \gradient{f(y)}^T (x - y) 
\]

Now let \( x, y \in M \) and  \( 0 \le \lambda \le 1 \). For \( z = \lambda x + (1 - \lambda) y \in M \) it follows that
\[
    \begin{split}
    \lambda f(x) & \ge \lambda f(z) + \lambda \gradient {f(z)}^T (x - z) \\
    (1 - \lambda) f(y) & \ge (1 - \lambda) f(z) + (1 - \lambda) \gradient {f(z)}^T (y - z)
    \end{split}
\]
Adding the two inequalities gives
\[
    \begin{split}
       \lambda f(x) + (1 - \lambda) f(y) 
        & \ge f(z) + \gradient {f(z)}^T(\lambda x - \lambda z + (1- \lambda) y - (1-\lambda)z) \\
        & = f(z) + \gradient {f(z)}^T(\lambda x + (1- \lambda) y - z) \\
        & = f(z)
    \end{split}
\]


\exercise{}
Let \( S \subset \mathbb{R}^n \) be compact and convex. Furthermore let \( f \in C^1(S) \) be convex,
\( x \in S \) and \( d \in \mathbb{R}^n \) a descent direction of \( f \) in \( x \) 
with \( \gradient {f(x)}^T d < 0 \).

\proof{}
If  \( x + \lambda^* d \) is an optimal solution then FONC holds and \( \gradient {f(x + \lambda^* d)}^T d = 0 \).
Let \( \gradient {f(x + \lambda^* d)}^T d = 0 \). Then Lemma\ref{lemma_gradient_inequality} gives
\[
    f(x + \lambda d) \ge f(x + \lambda^* d) + (\lambda - \lambda^*) \gradient {f(x + \lambda^* d)}^T d 
        = f(x + \lambda^* d) 
\]
and \( x + \lambda^* d \) is an optimal solution.

