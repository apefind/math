
\newpage
\section{Minimization without Constraints}

Alternative proof of Lemma 2.10.

\lemma[Gradient Inequality]\label{thm:lemma_gradient_inequality}
Let \(M \subseteq \mathbb{R}^n \) be a convex set and \(f \in C^1(M)\). Then \(f\) is convex if and only if
\[
    f(x) \ge f(y) + \gradient {f(y)}^T (x - y)
\]  
for all \(x, y \in M \). 

\proof{}
Let f be convex and \(x, y \in M\). For \( 0 \le \lambda \le 1 \) we have
\[ 
    f(\lambda x + (1 - \lambda) y) \le \lambda f(x) + (1 - \lambda)f(y) =  \lambda f(x) - \lambda f(y) + f(y) 
\] 
and 
\[ 
    f(x) - f(y) \ge \frac{f(\lambda x + (1 - \lambda) y) - f(y)}{\lambda}
        = \frac{f(y + \lambda (x - y)) - f(y)}{\lambda}
\]
For \( d = x - y \)\ and \( \lambda \to 0 \) the term on the right converges to the direction derivative of \( f \)
in \( d \)
\[
    \frac{\partial f}{\partial d}(y) = \gradient{f(y)}^T d = \gradient{f(y)}^T (x - y) 
\]

Now let \( x, y \in M \) and  \( 0 \le \lambda \le 1 \). For \( z = \lambda x + (1 - \lambda) y \in M \) it follows that
\[
    \begin{split}
        \lambda f(x) & \ge \lambda f(z) + \lambda \gradient {f(z)}^T (x - z) \\
            (1 - \lambda) f(y) & \ge (1 - \lambda) f(z) + (1 - \lambda) \gradient {f(z)}^T (y - z)
    \end{split}
\]
Adding the two inequalities gives
\[
    \begin{split}
       \lambda f(x) + (1 - \lambda) f(y) 
            & \ge f(z) + \gradient {f(z)}^T(\lambda x - \lambda z + (1- \lambda) y - (1-\lambda)z) \\
            & = f(z) + \gradient {f(z)}^T(\lambda x + (1- \lambda) y - z) \\
            & = f(z)
    \end{split}
\]


\exercise[Convex Functions]
The sum of convex functions is convex.

\proof{}
Let \( x, y \in M \). Since \( \alpha_i  > 0 \) we have
\[
    \begin{split}
        f(\lambda x + (1 - \lambda) y) 
            & = \sum_{i=1}^m \alpha_i f_i(\lambda x + (1 - \lambda) y)) \\
            & \le \sum_{i=1}^m \alpha_i \lambda f_i(x) + \sum_{i=1}^m \alpha_i (1 - \lambda) f_i(y) 
                 =  \lambda f(x) + (1 - \lambda) f(y) 
    \end{split}
\]
Let \( f(x) = x^2 \). Then \( -f \) is not convex, e.g. \( x = 1, y = -1 \) and \( \lambda = 0.5 \).


\exercise[Solution of Quadratic Inequality]
Let 
\[
    f(x) = x^{T}Ax + b^{T}x + c 
\]
\proof{}
The product rule gives 
\[ 
    \gradient {f(x)} = x^{T}A + Ax + b = (A^{T} + A)x + b = 2Ax + b 
\] 
Thus \( \gradient^2 {f(x)} = 2A > 0 \) and \( f \) is convex. Hence the level set \( \Gamma_{-c} \) is convex.
The intersection of convex sets is convex, so \( \Gamma_{-c} \cap \{ x \in \Rn: g^{T}x + h = 0 \} \)
is convex.

\exercise[Line Search on Compact Convex Sets]
Let \( S \subset \mathbb{R}^n \) be compact and convex. Furthermore let \( f \in C^1(S) \) be convex,
\( x \in S \) and \( d \in \mathbb{R}^n \) a descent direction of \( f \) in \( x \) 
with \( \gradient {f(x)}^T d < 0 \).

\proof{}
If  \( x + \lambda^* d \) is an optimal solution then FONC holds and \( \gradient {f(x + \lambda^* d)}^T d = 0 \).
Let \( \gradient {f(x + \lambda^* d)}^T d = 0 \). Then Lemma~\ref{thm:lemma_gradient_inequality} gives 
\[
    f(x + \lambda d) \ge f(x + \lambda^* d) + (\lambda - \lambda^*) \gradient {f(x + \lambda^* d)}^T d 
        = f(x + \lambda^* d) 
\]
and \( x + \lambda^* d \) is an optimal solution.


\exercise[Steepest Descent]
Let 
\[
       f(x) = \frac{1}{2} x^T{A}x + b^T x + c
\]
where \( A \) is symmetrical and positive definite. 

\proof{}
Since \( \gradient {f(x)} = Ax + b \) and \( \gradient^2 {f(x)} = A \ge 0 \) it follows \( x^* = -A^{-1}b \).
Let \( v \) be eigenvector with \( Av = \mu v \). Then for \( x_0 = x^* + \theta v \) we have
\( \gradient {f(x_0)} = Ax^* + \mu \theta v + b = \mu \theta v \) and
\[
    \begin{split}
        s_1         & = -\mu \theta v \\
        \lambda_1   & = \argmin \{f(x_0 + \lambda s_0)\}
                      = \argmin \{f(x^* + \theta v - \lambda \mu \theta v )\} = \mu^{-1} \\
        x_1         & = x_0 + \lambda_1 s_1 = x^* + \theta v - \mu^{-1} \mu \theta v = x^*
    \end{split}
\]
and hence \( \gradient {f(x_1)} = 0 \). So the algorithm  stops after the first iteration.
