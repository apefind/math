
\newpage
\section{Minimization without Constraints}

Alternative proof of Lemma 2.10.

\lemma{}
Let \(M \subseteq \mathbb{R}^n\) be a convex set and \(f \in C^1(M)\). Then \(f\) is convex if and only if
\[
    f(x) \ge f(y) + \gradient {f(y)}^T (x - y)
%    \label{eq:eq_lognormal}
\]
for all \(x, y \in M \). 

\proof{}
Let f be convex and \(x, y \in M\). For \( 0 \le \lambda \le 1 \) we have
\[ 
    f(\lambda x + (1 - \lambda) y) \le \lambda f(x) + (1 - \lambda)f(y) =  \lambda f(x) - \lambda f(y) + f(y) 
\] 
and 
\[ 
    f(x) - f(y) \ge \frac{1}{\lambda} (f(\lambda x + (1 - \lambda) y) - f(y)) 
        = \frac{f(y + \lambda (x - y)) - f(y)}{\lambda}
\]
For \( d = x - y \)\ and \( \lambda \to 0 \) the term on the right converges to the direction derivative of \( f \)
in \( d \)
\[
    \frac{\partial f}{\partial d}(y) = \gradient{f(y)}^T d = \gradient{f(y)}^T (x - y) 
\]

Now let \( x, y \in M \) and  \( 0 \le \lambda \le 1 \). For \( z = \lambda x + (1 - \lambda) y \in M \) it follows that
\[
    \begin{split}
    \lambda f(x) & \ge \lambda f(z) + \lambda \gradient {f(z)}^T (x - z) \\
    (1 - \lambda) f(y) & \ge (1 - \lambda) f(z) + (1 - \lambda) \gradient {f(z)}^T (y - z)
    \end{split}
\]
Adding the two inequalities gives
\[
    \begin{split}
       \lambda f(x) + (1 - \lambda) f(y) 
        & \ge f(z) + \gradient {f(z)}^T(\lambda x - \lambda z + (1- \lambda) y - (1-\lambda)z) \\
        & = f(z) + \gradient {f(z)}^T(\lambda x + (1- \lambda) y - z) \\
        & = f(z)
    \end{split}
\]
