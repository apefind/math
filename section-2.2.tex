
\subsection{One Dimensional Minimization and Direct Search}
\bigskip


\begin{definition}[Unimodal Function]\label{def:unimodal_fnc}
    A function \( f : [a,b] \to \R \) is called unimodal if there exists a \( \xi \in [a,b] \), so that
    \( f \) is strictly decreasing in \( [a, \xi] \) and strictly increasing in \( [\xi, b] \).
\end{definition}
\bigskip

In fact \( \xi \) is the unique minimum of \( f \) in \( [a, b] \). According to the definition,
for \( a \le x < y \le b \) we have
\[
    f(x) > f(y) \text{ for } x, y \in [a, \xi) \text{ and } f(x) < f(y) \text{ for }  x, y \in (\xi, b]   % chktex 9
\]
Thus
\[
    \xi \in [a, y] \text{ if } f(x) < f(y) \text{ and } \xi \in [x, b] \text{ if } f(x) \ge f(y)
\]

Consider now a symmetrical partioning of the interval \( [0, 1] \) where two consecutive partionings hold
the same ratio respectively:
\[
    \sigma = 1 - \tau \text{ and } \frac{1}{\tau} = \frac{\tau}{\sigma}
\]
Then \( 1 - \tau = \tau^2 \) and solving the quadratic equation \( \tau^2 + \tau = 1 \) yields
\[
    \tau = \frac{\sqrt{5} - 1}{2} \approx 0.61803
\]
\bigskip

\begin{figure}[H]
    \centering
    \plotgoldensection{}
    \caption{Golden Section}\label{fig:golden_section}
\end{figure}
\bigskip

Let now \( [a_0, b_0] = [a, b] \) and define
\[
    [a_{k + 1}, b_{k + 1}] =
    \begin{cases}
        [a_k, y_k] & \text{ if } f(x_k) < f(y_k)   \\
        [x_k, b_k] & \text{ if } f(x_k) \ge f(y_k)
    \end{cases}
\]
where
\[
    \begin{split}
        x_k & = b_k - \tau (b_k - a_k) \\
        y_k & = a_k + \tau (b_k - a_k)
    \end{split}
\]
It follows that \( [a_k, b_k] \supset [a_{k + 1}, b_{k + 1}] \) is a decreasing series of intervals with
\[
    (b_{k + 1} - a_{k + 1}) =  \tau(b_k - a_k)
\]
where the interval converges to \( \xi \). This leads to the following algorithm:
\bigskip

\begin{algorithm}[Golden Section Search]\label{algo:golden_section_search}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/golden_section.py}
\bigskip

\begin{exercise}[Surprising Convergence]
    Example for \( f \in C^2(\mathbb{R}) \) with a sequence of strict local minima converging to a strict local maximum.
\end{exercise}
\bigskip



\subsection{Methods of Steepest Descent}
\bigskip


\begin{definition}
    Let \( f \in C^1(\Rn) \) and \( x_0 \in \Rn \) an arbitraty starting point.
    \begin{enumerate}
        \item For sequences \( \lambda_k > 0 \) and unit vectors \( s_k \in \Rn \) define
              \[
                  x_{k + 1} = x_k + \lambda_k s_k
              \]
        \item Assume there exists a \( 0 < \alpha \le 1 \) so that
              \[
                  - \gradient f(x_k) s_k \ge \alpha \|\gradient f(x_k)\|
              \]
        \item Furthermore assume that for some \( 0 < \beta \le \gamma < 1 \) the following inequalities hold
              \[
                  \begin{split}
                      f(x_{k + 1}) & \le f(x_k) + \lambda_k \beta \gradient f(x_k) s_k \\
                      \gradient f(x_{k + 1}) s_k & \ge \gamma \gradient f(x_k) s_k
                  \end{split}
              \]
    \end{enumerate}
    Then \( \lambda_k \) and  \( s_k \) are called \emph{step lengths} and \emph{search directions},
    \( x_k \) is called a \emph{sequence of descent} for \( f \).
\end{definition}
\bigskip


\begin{remarks}\hfill
    \begin{enumerate}
        \item  It is
              \[
                  - \gradient f(x_k) s_k = \cos\varphi \|\gradient f(x_k)\| \|s_k\| =
                  \cos\varphi \|\gradient f(x_k)\| \ge \alpha \|\gradient f(x_k)\|
              \]
              Hence angle between the direction of the steepest descent \( -\gradient f(x_k) \)
              and \( s_k \) is strictly smaller then \( \ang{90} \) degrees.
        \item  Since \( \gradient f(x_k) s_k \le 0 \) it follows
              \[
                  f(x_{k + 1}) \le f(x_k) + \beta \lambda_k \gradient f(x_k) s_k \le f(x_k)
              \]
              and \( f(x_k) \) is monotonically decreasing.
        \item Furthermore
              \[
                  \gradient f(x_{k + 1}) s_k = \gradient f(x_k + \lambda_k s_k) s_k
                  \ge \gamma \gradient f(x_k) s_k
              \]
              Hence the step length cannot be chosen too small due to the continuity of the dervative.

    \end{enumerate}
\end{remarks}
\bigskip


\begin{theorem}[Steepest Descent Method]\label{thm:steepest_descent}
    Let \( f \in C^2(\Rn) \) and \( x_k \) a sequence of descent.
    Then every accumulation point of \( x_k \) is a stationary point of \( f \).
\end{theorem}

\begin{proof}
\end{proof}
