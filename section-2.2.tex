
\subsection{One Dimensional Minimization and Direct Search}
\bigskip


\begin{definition}[Unimodal Function]\label{def:unimodal_fnc}
    A function \( f : [a,b] \to \R \) is called unimodal if there exists a \( \xi \in [a,b] \), so that
    \( f \) is strictly decreasing in \( [a, \xi] \) and strictly increasing in \( [\xi, b] \).
\end{definition}
\bigskip

In fact \( \xi \) is the unique minimum of \( f \) in \( [a, b] \). According to the definition,
for \( a \le x < y \le b \) we have
\[
    f(x) > f(y) \text{ for } x, y \in [a, \xi) \text{ and } f(x) < f(y) \text{ for }  x, y \in (\xi, b]   % chktex 9
\]
Thus
\[
    \xi \in [a, y] \text{ if } f(x) < f(y) \text{ and } \xi \in [x, b] \text{ if } f(x) \ge f(y)
\]

Consider now a symmetrical partioning of the interval \( [0, 1] \) where two consecutive partionings hold
the same ratio respectively:
\[
    \sigma = 1 - \tau \text{ and } \frac{1}{\tau} = \frac{\tau}{\sigma}
\]
Then \( 1 - \tau = \tau^2 \) and solving the quadratic equation \( \tau^2 + \tau = 1 \) yields
\[
    \tau = \frac{\sqrt{5} - 1}{2} \approx 0.61803
\]
\bigskip

\begin{figure}[H]
    \centering
    \plotgoldensection{}
    \caption{Golden Section}\label{fig:golden_section}
\end{figure}
\bigskip

Let now \( [a_0, b_0] = [a, b] \) and define
\[
    [a_{k + 1}, b_{k + 1}] =
    \begin{cases}
        [a_k, y_k] & \text{ if } f(x_k) < f(y_k)   \\
        [x_k, b_k] & \text{ if } f(x_k) \ge f(y_k)
    \end{cases}
\]
where
\[
    \begin{split}
        x_k & = b_k - \tau (b_k - a_k) \\
        y_k & = a_k + \tau (b_k - a_k)
    \end{split}
\]
It follows that \( [a_k, b_k] \supset [a_{k + 1}, b_{k + 1}] \) is a decreasing series of intervals with
\[
    (b_{k + 1} - a_{k + 1}) =  \tau(b_k - a_k)
\]
where the interval converges to \( \xi \). This leads to the following algorithm:
\bigskip

\begin{algorithm}[Golden Section Search]\label{algo:golden_section_search}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/golden_section.py}
\bigskip

\begin{exercise}[Surprising Convergence]
    Example for \( f \in C^2(\mathbb{R}) \) with a sequence of strict local minima converging to a strict local maximum.
\end{exercise}
\bigskip



\subsection{Methods of Steepest Descent}
\bigskip


\begin{definition}
    Let \( f \in C^1(\Rn) \) and \( x_0 \in \Rn \). For sequences \( \lambda_k > 0 \) and
    unit vectors \( s_k \in \Rn \) define
    \[
        x_{k + 1} = x_k + \lambda_k s_k
    \]
    Then \( \lambda_k \) and  \( s_k \) are called \emph{step lengths} and \emph{search directions} if there
    exists a \( 0 < \gamma \le 1 \), so that
    \[
        - \gradient f(x_k) s_k \ge \gamma \|\gradient f(x_k)\|
    \]
\end{definition}
\bigskip


\begin{remark}
    It is
    \[
        - \gradient f(x_k) s_k = \|\gradient f(x_k)\| \|s_k\| \cos\varphi =
        \cos\varphi \|\gradient f(x_k)\| \ge \gamma \|\gradient f(x_k)\|
    \]
    and so angle between \( -\gradient f(x_k) \) and \( s_k \) is strictly smaller then \( \ang{90} \) degrees
\end{remark}
\bigskip


\begin{theorem}[Steepest Descent Method]\label{thm:steepest_descent}
    Let \( f \in C^1(\Rn) \) as well as \( \lambda_k \) and \( s_k \) step lengths and search directions.
    Furthermore assume that for some \( 0 < \alpha \le \beta < 1 \) the following inequalities hold
    \[
        \begin{split}
            f(x_{k + 1}) & \le f(x_k) + \alpha \gamma_k \gradient f(x_k) s_k \\
            \gradient f(x_{k + 1}) s_k & \ge \beta f(x_k) s_k
        \end{split}
    \]
    Then every accumulation point of \( x_k \) is a stationary point of \( f \).
\end{theorem}

\begin{proof}
    Since \( \gradient f(x_k) s_k \le 0 \) the sequence \( f(x_k) \) is monotonically decreasing.
\end{proof}
