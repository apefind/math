\newpage
\section{Neural Networks}

\subsection{The Perceptron}
\bigskip

\begin{definition}[Binary Classifiers]
    Let \( X \subset \R^n \) be the union of two finite disjoint sets \( X = M \cup N \).
    \begin{enumerate}
        \item A \emph{binary classification problem} is the task to find a mapping \( f: X \to \{ 0, 1 \} \) with
              \[
                  f(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ for } x \in M \\
                      0 & \text{ for } x \in N
                  \end{array}
                  \right.
              \]
              \( f \) then is called a \emph{binary classifier} for \( X \)
        \item \( X \) is called \emph{separable} if there exists a \emph{weight vector} \( w \in \R^n \)
              and a \emph{bias} \( b \in \R \) so that
              \[
                  \begin{split}
                      wx + b & > 0 \hspace{1em}\text{ for } x \in M \\
                      wx + b & < 0 \hspace{1em}\text{ for } x \in N
                  \end{split}
              \]
        \item The weight \( w \) and the bias \( b \) are called \emph{solution to the classification problem}.
              They implicitly define a binary classifier via
              \[
                  f(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ if } wx + b > 0 \\
                      0 & \text{ if } wx + b < 0
                  \end{array}
                  \right.
              \]
    \end{enumerate}
\end{definition}
\bigskip


\begin{examples}
    \hfill
    \begin{enumerate}
        \item Let \( X = \{ 0, 1 \} \times \{ 0, 1 \} \) and consider the \emph{and} operator
              \( f(1, 1) = 1 \) and \( f(x, y) = 0 \) elsewhere. Then \( w = (3, 3) \) and \( b = -5 \)
              yield a solution to the classification problem \( M = f^{-1}(1) \) and \( N = f^{-1}(0) \)
        \item Again let \( X = \{ 0, 1 \} \times \{ 0, 1 \} \) and \( f(1, 0) = f(0, 1) = 1 \) and
              \( f(0, 0) = f(1, 1) = 0 \), the \emph{xor} operator. Thus for any weight \( (w_1, w_2) \) and
              any bias \( b \)
              \[
                  \begin{split}
                      w_1 + b & > 0 \\
                      w_2 + b & > 0 \\
                      \\
                      w_1 + w_2 + b & \le 0 \\
                      b & \le 0
                  \end{split}
              \]
              Adding two equations respectively shows that there cannot be a solution
        \item The bias can be integrated into the weight vector via \( w' = (w, b) \in \R^{n + 1} \) and
              \( x' = (x, 1) \in \R^{n + 1} \). Separability then reduces to
              \[
                  w'x' > 0
              \]
    \end{enumerate}
\end{examples}
\bigskip


\subsubsection*{Geometrical Interpretation}
\bigskip
The idea for the perceptron most likely has its origin in a simple geometrical observation.
Recall that for \( x, y \in \R^n \) the dot product can be expressed as
\[
    xy = \|x\| \|y\| \cos\alpha
\]
where \( \alpha \) is the angle between the two vectors. Hence the product is positive
if the angle is less than \( \ang{90} \) degrees and negative if the angle is between \( \ang{90} \)
and \( \ang{180} \) degrees
\[
    \begin{split}
        xy & > 0 \hspace{1em}\text{ for } 0 \le \alpha < \pi / 2 \\
        xy & < 0 \hspace{1em}\text{ for } \pi / 2 < \alpha \le \pi
    \end{split}
\]
Note, that the sign does not depend on the vector lengths, but solely on the angle.

For any two vectors it is easy enough to find a weight that satisfies \( wx > 0 \) and \( wy > 0 \).
Generally \( w = x + y \) is a good guess, but not always correct as shown below in
\hyperref[fig:vectorangle]{Figure~\ref*{fig:vectorangle}}.

\bigskip
\begin{figure}[H]
    \centering
    \plotvectorangle{}
    \caption{Dot Product and Angle}\label{fig:vectorangle}
\end{figure}
\bigskip

But, the more similar the lengths of the two vectors are the more likely \( x + y \) works.
The actual threshold is given by the following
\bigskip

\begin{lemma}
    Let \( x, y \in \Rn \) with \( \|x\| < \|y\| \) and \( xy = \|x\| \|y\| \cos\alpha \). If
    \[
        \|x\| > -\|y\|\cos\alpha
    \]
    then \( x(x + y) > 0 \).
\end{lemma}

\begin{proof}
    Let \( x(x + y) = \|x\| \|x + y\| \cos\beta \). Since
    \[
        x(x + y) = xx + xy = {\|x\|}^2 + \|x\| \|y\| \cos\alpha
    \]
    it follows
    \[
        {\|x + y\|}\cos\beta = \|x\| + \|y\|\cos\alpha
    \]
    Hence \( \cos\beta > 0 \) if the inequality above holds.
\end{proof}
\bigskip


An iterative approach is to repeatedly increase \( w = x + y \) in the direction of the shorter vector
aka the one with the angle greater than \( \ang{90} \) degrees.
\[
    w' = \left \{
    \begin{array}{ll}
        w + x & \text{ if } w x \le 0 \\
        w + y & \text{ if } w y \le 0 \\
    \end{array}
    \right.
\]

While this seems reasonable it is unclear whether the algorithm always yields a result after
a finite number of iterations. The answer to this question will be given later as a special case
of the \hyperref[thm:perceptron_convergence]{Perceptron Convergence Theorem~\ref*{thm:perceptron_convergence}}.

This approach can be used to to separate two vectors. Finding a common weight for  \( x \) and \( -y \)
now yields \( wx > 0 \) and \( -wy > 0 \), hence \( wy < 0 \).
\[
    w' = \left \{
    \begin{array}{ll}
        w + x & \text{ if } w x \le 0 \\
        w - y & \text{ if } w y \ge 0 \\
    \end{array}
    \right.
\]
\bigskip


\begin{algorithm}[Weight]\label{algo:weight}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/weight.py}
\bigskip


\begin{examples}
    \hfill
    \begin{enumerate}
        \item Let \( x = (-1, 1) \) and \( y = (6, 1) \). Then
              \[
                  \begin{aligned}
                      w_0 & = (5, 2) & w_0x & = -3 & w_0y & = 32 \\
                      w_1 & = (4, 3) & w_1x & = -1 & w_1y & = 27 \\
                      w_2 & = (3, 4) & w_2x & = 1  & w_2y & = 22 \\
                  \end{aligned}
              \]
        \item Let \( x = (4, -6) \) and \( y = (-10, 5) \). Then
              \[
                  \begin{aligned}
                      w_0 & = (-6, -1)  & w_0x & = -18 & w_0y & = 55  \\
                      w_1 & = (-2, -7)  & w_1x & = 34  & w_1y & = -15 \\
                      w_2 & = (-12, -2) & w_2x & = -36 & w_2y & = 110 \\
                      w_3 & = (-8, -8)  & w_3x & = 16  & w_3y & = 40  \\
                  \end{aligned}
              \]
        \item Task: for any given integer \( k \) find two vectors so that more than \( k \) steps are needed
        \item Let \( w = x / \|x\| + y/ \|y\| \). Then
              \[
                  wx = \frac{{\|x\|}^2}{\|x\|} + \frac{yx}{\|y\|} = \|x\| + \|x\|\cos\alpha = (1 + \cos\alpha)\|x\| > 0
              \]
              and on the other hand \( wx = \|w\| \|x\| \cos\beta \). Similarily
              \( wy = (1 + \cos\alpha)\|y\| = \|w\| \|y\| \cos\gamma \). Hence
              \[
                  1 + \cos\alpha = \|w\|\cos\beta = \|w\|\cos\gamma
              \]
              and thus \( \beta = \gamma \). Also, as expected, \( wx > 0 \) and \( wy > 0 \)
    \end{enumerate}
\end{examples}
\bigskip


The following theorem is the generalization of the approach above for binary classifiers.
Roughly sepaking the algorithm changes the weight only for misqualified vectors. The proof measures
the change of angle against the change of length of the weight vector and provides an estimation
for the maximumn number of required steps.
\bigskip

\begin{theorem}[Perceptron Convergence Theorem]\label{thm:perceptron_convergence}
    Let \( X = M \cup N \) be separable by \( w^*\in \Rn \). Define \( w_0 = 0 \)
    and repeat to iterate over all \( x \in X \) via
    \[
        w_{k + 1} = \left \{
        \begin{array}{ll}
            w_k + x & \text{ if } x \in M \text{ and } w_k x \le 0 \\
            w_k - x & \text{ if } x \in N \text{ and } w_k x \ge 0 \\
            w_k     & \text{ else }                                \\
        \end{array}
        \right.
    \]
    until no further changes occurr. Suppose \( \|x\| \le r \)  and \( |w^*x| \ge \delta > 0 \)
    for \( x \in X \). Then the number of iterations before the algorithm stops is limited by
    \[
        k\le \frac{r^2}{\delta^2}
    \]
\end{theorem}

\begin{proof}
    Let \( x \in M \) with \( w_k x \le 0 \). Then
    \[
        w^*w_{k + 1} = w^*w_k + w^*x \ge w^*w_k + \delta
    \]
    Furthermore
    \[
        \|w_{k + 1}\|^2 = \|w_k + x\|^2 = \|w_k\|^2 + 2w_k x + \|x\|^2 \le \|w_k\|^2 + r^2
    \]

    The same estimations holds for \( x \in N \) with \( w_k x \ge 0 \) and using induction yields
    \[
        w^*w_k \ge k\delta \text{ and } \|w_k\|^2 \le kr^2
    \]
    Assuming \( \|w^*\| = 1\) now gives
    \[
        k^2\delta^2 \le \|w_k\|^2 \le kr^2
    \]
    which proves the initial inequality.
\end{proof}
\bigskip


\begin{algorithm}[Perceptron]\label{algo:perceptron}
\end{algorithm}
\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/perceptron.py}
\bigskip


\begin{remark}
    Check \hyperref[thm:banach_fix_point]{Banach Fixed-Point Theorem~\ref*{thm:banach_fix_point}}
    for an alternative proof?!
\end{remark}
\bigskip


\subsection{Single Layer Feedforward Neural Networks and the Delta Rule}
\bigskip

\begin{definition}[Activation Function]
    \hfill
    \begin{enumerate}
        \item There is no valuable formal definition for an \emph{activation function}
              \( \alpha: \R \to Y \subseteq \R \). However, the behaviour of an activation function should
              somehow relate to the fireing behaviour of an actual biological neuron. Continuity,
              differentiability, monotony and the behaviour at infinity are of special interest in order
              to apply mathematical concepts
        \item An activation function \( \alpha: \R \to \R \) with \( \alpha(x) \to 0 \)
              for \( x \to -\infty \) and \( \alpha(x) \to 1 \) for \( x \to \infty \) is called a
              \emph{sigmoidal activation function}
        \item The \emph{Heaviside} function \( H: \R \to \{ 0, 1 \} \) is defined as
              \[
                  H(x) = \left \{
                  \begin{array}{ll}
                      1 & \text{ for } x > 0  \\
                      0 & \text{ for }x \le 0
                  \end{array}
                  \right.
              \]
        \item The \emph{sigmoid} function \( \sigmoid \in C^\infty(\R) \) is defined as
              \[
                  \sigmoid(x) = \frac{1}{1 + e^{-x}}
              \]
    \end{enumerate}
\end{definition}
\bigskip


\begin{remarks}
    \hfill
    \begin{enumerate}
        \item The heaviside function is not continuous and therefore not differentiable at \( 0 \).
              In that sense the sigmoid function can be considered its smooth counterpart
        \item The definition of the sigmoid function yields \( 0 < \sigmoid(x) < 1 \) as well as
              \( \sigmoid(x) \to 0 \) for \( x \to -\infty \) and \( \sigmoid(x) \to 1 \)
              for \( x \to \infty \)
        \item The quotient rule yields
              \[
                  \sigmoid'(x)
                  = -\frac{-e^{-x}}{{(1 + e^{-x})}^2}
                  = \sigmoid(x) \frac{1 + e^{-x} - 1}{1 + e^{-x}}
                  = \sigmoid(x)(1 - \sigmoid(x))
              \]
              and \( \sigmoid \) is monotonically increasing over its domain
    \end{enumerate}
\end{remarks}
\bigskip


\begin{figure}[H]
    \centering
    \plotsigmoid{}
    \caption{The sigmoid function \( \sigmoid(x) = \frac{1}{1 + e^{-x}} \)}\label{fig:sigmoid}
\end{figure}
\bigskip

\begin{remarks}
    \hfill
    \begin{enumerate}
        \item Let \( f \in C^1(\Rn) \) and \( g(x) = f(x|^2 \). The multivariable  chain rule yields
              \[
                  \frac{\partial g}{\partial x_i}(x)
                  = \frac{\partial f^2}{\partial x_i}(x)
                  = 2 f(x) \frac{\partial f}{\partial x_i}(x)
              \]
              and
              \[
                  \gradient g(x) = 2 f(x) \gradient f(x)
              \]
        \item More generally for \( f = (f_1, f_2, \dots f_n)\in C^1(\Rn,\Rm) \) and
              \[
                  g(x) = \|f(x)\|^2 = \sum_{j = 1}^m {f_j(x)}^2
              \]
              it follows that
              \[
                  \frac{\partial g_j}{\partial x_i}(x)
                  = \sum_{j = 1}^m \frac{\partial f_j^2}{\partial x_i}(x)
                  = 2 \sum_{j = 1}^m \frac{\partial f_j}{\partial x_i}(x) f_j(x)
              \]
              and
              \[
                  \gradient g(x) = 2 \sum_{j = 1}^m \gradient f_j(x) f_j(x)
              \]
        \item Now let \( x \in \Rn \) and \( y \in R \) be fixed. For \( q(w, b) = wx + b - y \) follows
              \[
                  \begin{split}
                      \frac{\partial q}{\partial w_i}(w, b) &=
                      \frac{\partial}{\partial w_i} \sum_{i = 0}^n w_i x_i + b - y = x_i \\
                      \frac{\partial q}{\partial b}(w, b) &= 1
                  \end{split}
              \]
              Hence
              \[
                  \gradient q(w, b) = (x_1, x_2, \dots, x_n, 1)
              \]
        \item Let \( \alpha \in C^1(\R) \) be an activation function and define
              \( p(w,b) = (y - \alpha(wx + b))^2 \).
              Then
              \[
                  \begin{split}
                      \frac{\partial p}{\partial w_i}(w, b) &= -2 (y - \alpha(wx + b)) \alpha'(wx + b) x_i  \\
                      \frac{\partial p}{\partial b}(w, b) &= -2 (y - \alpha(wx + b)) \alpha'(wx + b)
                  \end{split}
              \]
              and
              \[
                  \gradient p(w, b) = -2 (y - \alpha(wx + b)) \alpha'(wx + b)\gradient q(w, b)
              \]
    \end{enumerate}
\end{remarks}
\bigskip


\begin{definition}[Single Layer Feedforward Neural Network]
    Let \( \alpha \in C^1(\R) \) be an activation function, \( w \in \R^n \)  and \( b \in \R \)
    be weight and bias. Define \( f \in C^1(\Rn) \) as
    \[
        f(x) = \alpha(w x + b)
    \]
    Then \( f = \alpha(wx + b) \) is called a \emph{single layer feedforward neural network}.
\end{definition}
\bigskip


\begin{definition}[Error Function]
    Let \( f = \alpha(wx + b) \) be a single layer feedforward neural network.
    \begin{enumerate}
        \item A finite set \( T \subset \Rn \times \R \) is called  \emph{test set}
        \item The \emph{error function} is defined as the mean squared error over the samples
              \[
                  E(w,b) = \frac{1}{2} \sum_{(x,y) \in T} {(y - f(x))}^2
              \]
    \end{enumerate}
\end{definition}
\bigskip


\begin{lemma}
    It is
    \[
        \begin{split}
            \frac{\partial E}{\partial w_i}(w, b) &= \sum_{(x,y) \in T} - (y - f(x)) \alpha'(wx + b) x_i\\
            \frac{\partial E}{\partial b}(w, b) &= \sum_{(x,y) \in T} - (y - f(x)) \alpha'(wx + b)
        \end{split}
    \]
\end{lemma}

\begin{proof}
    This follows from the remarks above.
\end{proof}
\bigskip


The Delta Rule applies the method of the Steepest Descent to the error function of a neural network.
\bigskip


\begin{algorithm}[Delta Rule]\label{algo:delta_rule}
    Let \( f = \alpha(wx + b) \) be a single layer feedforward neural network
    and \( T \) a test set.
    The delta rule modifies weight and bias in the direction of the greatest descent \( - \eta \gradient E(w,b) \)
    at a given \emph{learning rate} \( \eta > 0 \).  For \( (x, y) \in T \) this is
    \[
        \begin{split}
            \Delta w_i & = \eta (y - f(x)) \alpha'(wx + b) x_i \\
            \Delta b & = \eta (y - f(x)) \alpha'(wx + b)
        \end{split}
    \]
\end{algorithm}
\bigskip


\inputminted[fontsize=\small, framesep=0.35cm, frame=lines, python3=true]{python}{python/delta_rule.py}
\bigskip


\subsection{The Univeral Approximation Theorem}
\bigskip

% \begin{definition}[Single Layer Neural Network]
%     Let \( w_k \in \R^n \) and \( b_k \in \R \). For \( \alpha_k \in \R \) define \( f : \Rn \to \R \)
%     \[
%         f(x) = \sum_{k = 0}^m \alpha_k \sigmoid(w_k x + b_k)
%     \]
%     Then \( f \) is called a \emph{single layer feedforward neural network} of \( m \) units.
% \end{definition}
% \bigskip

\begin{theorem}[The Universal Approximation Theorem]\label{thm:universal_approximation}
    The subspace of all single layer feedforward neural networks is dense in \( C(I^n) \).
\end{theorem}


%\subsection{The Backpropagation Algorithm}
