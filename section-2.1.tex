
\newpage
\section{Nonlinear Optimization}
\subsection{Minimization without Constraints}


\begin{lemma}[Gradient Inequality]\label{thm:lemma_gradient_inequality}
Let \(M \subseteq \mathbb{R}^n \) be a convex set and \(f \in C^1(M)\). Then \(f\) is convex if and only if
\[
    f(x) \ge f(y) + \gradient {f(y)}^T (x - y)
\]  
for all \(x, y \in M \). 
\end{lemma}

\begin{proof}
Let f be convex and \(x, y \in M\). For \( 0 \le \lambda \le 1 \) we have
\[ 
    f(\lambda x + (1 - \lambda) y) \le \lambda f(x) + (1 - \lambda)f(y) =  \lambda f(x) - \lambda f(y) + f(y) 
\] 
and 
\[ 
    f(x) - f(y) \ge \frac{f(\lambda x + (1 - \lambda) y) - f(y)}{\lambda}
        = \frac{f(y + \lambda (x - y)) - f(y)}{\lambda}
\]
For \( d = x - y \)\ and \( \lambda \to 0 \) the term on the right converges to the direction derivative of \( f \)
in \( d \)
\[
    \frac{\partial f}{\partial d}(y) = \gradient{f(y)}^T d = \gradient{f(y)}^T (x - y) 
\]

Now let \( x, y \in M \) and  \( 0 \le \lambda \le 1 \). For \( z = \lambda x + (1 - \lambda) y \in M \) it follows that
\[
    \begin{split}
        \lambda f(x) & \ge \lambda f(z) + \lambda \gradient {f(z)}^T (x - z) \\
            (1 - \lambda) f(y) & \ge (1 - \lambda) f(z) + (1 - \lambda) \gradient {f(z)}^T (y - z)
    \end{split}
\]
Adding the two inequalities gives
\[
    \begin{split}
       \lambda f(x) + (1 - \lambda) f(y) 
            & \ge f(z) + \gradient {f(z)}^T(\lambda x - \lambda z + (1- \lambda) y - (1-\lambda)z) \\
            & = f(z) + \gradient {f(z)}^T(\lambda x + (1- \lambda) y - z) \\
            & = f(z)
    \end{split}
\]
\end{proof}
\bigskip


\begin{exercise}[Facility Locations]
The facilities are located at:
\[
    (3, 0), (0, -3), (1, 4) 
\]
\end{exercise}

\begin{proof}
Let 
\[
    \begin{split}
        f(x)    & = {(x - 3)}^2 + y^2 + x^2 + {(y + 3)}^2 + {(x - 1)}^2 + {(y - 4)}^2 \\
                & = x^2 - 6x + 9 + y^2 + x^2 + y^2 + 6y + 9 + x^2 - 2x + 1 + y^2 -8y + 16 \\
                & = 3x^2 + 3y^2 - 8x - 2y + 35
    \end{split}
\]
Then
\[
    \gradient f(x, y) = (6x - 8, 6y - 2) \text{ and } \gradient^2 f(x, y) = 
        \begin{pmatrix}
            6 & 0 \\
            0 & 6 \\
        \end{pmatrix} > 0
\]
Hence \( (4 / 3, 1 / 3) \) is the gobal minimum. 
\end{proof}
\bigskip


\exercise[Convex Functions]
The sum of convex functions is convex.

\proof{}
Let \( x, y \in M \). Since \( \alpha_i  > 0 \) we have
\[
    \begin{split}
        f(\lambda x + (1 - \lambda) y) 
            & = \sum_{i=1}^m \alpha_i f_i(\lambda x + (1 - \lambda) y)) \\
            & \le \sum_{i=1}^m \alpha_i \lambda f_i(x) + \sum_{i=1}^m \alpha_i (1 - \lambda) f_i(y) 
                 =  \lambda f(x) + (1 - \lambda) f(y) 
    \end{split}
\]
Let \( f(x) = x^2 \). Then \( -f \) is not convex, e.g. \( x = 1, y = -1 \) and \( \lambda = 0.5 \).
\bigskip


\exercise[Solution of Quadratic Inequality]
Let 
\[
    f(x) = x^{T}Ax + b^{T}x + c 
\]
\proof{}
The product rule gives 
\[ 
    \gradient {f(x)} = x^{T}A + Ax + b = (A^{T} + A)x + b = 2Ax + b 
\] 
Thus \( \hessian {f(x)} = 2A > 0 \) and \( f \) is convex. Hence the level set \( \Gamma_{-c} \) is convex.
Since the intersection of convex sets is convex \( \Gamma_{-c} \cap \{ x \in \Rn: g^{T}x + h = 0 \} \) is convex, too.
\bigskip


\exercise[Line Search on Compact Convex Sets]
Let \( S \subset \mathbb{R}^n \) be compact and convex. Furthermore let \( f \in C^1(S) \) be convex,
\( x \in S \) and \( d \in \mathbb{R}^n \) a descent direction of \( f \) in \( x \) 
with \( \gradient {f(x)}^T d < 0 \).

\proof{}
If  \( x + \lambda^* d \) is an optimal solution then \( \gradient {f(x + \lambda^* d)}^T d = 0 \) 
according to Theorem~\ref{thm:fonc}.
Let \( \gradient {f(x + \lambda^* d)}^T d = 0 \). Then Lemma~\ref{thm:lemma_gradient_inequality} gives 
\[
    f(x + \lambda d) \ge f(x + \lambda^* d) + (\lambda - \lambda^*) \gradient {f(x + \lambda^* d)}^T d 
        = f(x + \lambda^* d) 
\]
and \( x + \lambda^* d \) is an optimal solution.
\bigskip


\begin{exercise}[Steepest Descent]
Let 
\[
       f(x) = \frac{1}{2} x^T{A}x + b^T x + c
\]
where \( A \) is symmetrical and positive definite. 
\end{exercise}

\begin{proof}
Since \( \gradient {f(x)} = Ax + b \) and \( \hessian {f(x)} = A > 0 \) it follows \( x^* = -A^{-1}b \).
Let \( v \) be eigenvector with \( Av = \mu v \). For \( x_0 = x^* + \theta v \) we have
\[ 
    \gradient {f(x_0)} = Ax^* + \mu \theta v + b = \mu \theta v 
\] 
and for \(\lambda \ge 0 \)
\[
        \argmin \{ f(x_0 - \lambda \gradient f(x_0) \} =\argmin \{ f(x^* + \theta v - \lambda \mu \theta v)\} = \mu^{-1} 
\]
Thus         
\[
    x_1 = x_0 - \mu^{-1} \gradient f(x_0) = x^* + \theta v - \mu^{-1} \mu \theta v = x^*
\]
and \( \gradient {f(x_1)} = 0 \). Hence the algorithm stops after the first iteration.
Now let 
\[
    x_0 = x^* + \sum_{i=0}^m \theta_i v_i
\]
for orthogonal eigenvectors with \( Av_i = \mu_i\) and \( m \le n \). Then
\[ 
    \gradient {f(x_0)} = Ax^* + \sum_{i=0}^m \mu_i \theta_i v_i + b = \sum_{i=0}^m \mu_i \theta_i v_i
\]
and
\[ 
    x_1 = x_0 - \lambda \sum_{i=0}^m \mu_i \theta_i v_i 
        = x^* + \sum_{i=0}^m \theta_i v_i - \lambda \sum_{i=0}^m \mu_i \theta_i v_i
        = x^* + \sum_{i=0}^m (1 - \lambda \mu_i) \theta_i v_i
\]
Since \( x^* \) is the minimum we have \( \gradient {f(x_1)} = 0 \) iff \( \lambda = \mu^{-1} \) for all \( 0 \le i \le m \).
\end{proof}
\bigskip
